# -*- coding: utf-8 -*-
"""NLP - posTags.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SeY0fXMllwen2KxmYEpRkymvJyik9Y5y
"""


import pandas as pd
import numpy as np
import requests, zlib
import warnings

from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn import model_selection
from sklearn.utils import shuffle
from sklearn.metrics import accuracy_score

from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB as MNB
from sklearn.neural_network import MLPClassifier as MLPC
from sklearn.linear_model import LogisticRegression as LR
from sklearn.ensemble import RandomForestClassifier as RFC
from sklearn.ensemble import AdaBoostClassifier as ABC
from sklearn.ensemble import BaggingClassifier as BC
from sklearn.tree import DecisionTreeClassifier as DTC
from scipy.sparse import hstack

import nltk
from nltk import word_tokenize
nltk.download('all')

warnings.filterwarnings("ignore")

# get data

clickbait = pd.read_csv('https://raw.githubusercontent.com/MarynaLongnickel/sklearn-intro/master/clickbait.txt', delimiter="\t", header = None)
instances = pd.read_json('https://raw.githubusercontent.com/MarynaLongnickel/sklearn-intro/master/instances.jsonl', lines=True)
truth = pd.read_json('https://raw.githubusercontent.com/MarynaLongnickel/sklearn-intro/master/truth.jsonl', lines=True)

res = requests.get('https://github.com/bhargaviparanjape/clickbait/raw/master/dataset/clickbait_data.gz')
data = zlib.decompress(res.content, zlib.MAX_WBITS|32)
data = data.decode("utf-8").split('\n\n')
clickbait = data[:-1]


res2 = requests.get('https://github.com/bhargaviparanjape/clickbait/raw/master/dataset/non_clickbait_data.gz')
data2 = zlib.decompress(res2.content, zlib.MAX_WBITS|32)
data2 = data2.decode("utf-8").split('\n\n')
non_clickbait = data2[:-1]

pd.set_option('display.max_colwidth', 1000)

instances = instances[['id', 'targetTitle']]

truth = truth[['id', 'truthClass']]

cc = pd.merge(instances, truth, on='id')
cc['isClickbait'] = cc['truthClass'].map({'no-clickbait': '0', 'clickbait': '1'})
cc = cc[['targetTitle', 'isClickbait']]
cc.columns = ['text', 'isClickbait']

df = pd.DataFrame(clickbait)
df2 = pd.DataFrame(non_clickbait)

df.columns = ['text']
df['isClickbait'] = ['1']*len(clickbait)

df2.columns = ['text']
df2['isClickbait'] = ['0']*len(non_clickbait)

# shuffle before dividing into training and testing sets
# (clickbait challenge data included)

data = pd.concat([df, df2, cc], axis = 0)
data = shuffle(data)

split = int(len(data)*0.75)

vectorizer = TfidfVectorizer()
# count_vectorizer = CountVectorizer()
# bigram_vectorized = CountVectorizer(ngram_range=(1,2))
svm = LinearSVC()

X_train = data['text'][:split].values
y_train = data['isClickbait'][:split].values
X_test = data['text'][split:].values
y_test = data['isClickbait'][split:].values

X_train = vectorizer.fit_transform(X_train)
X_test = vectorizer.transform(X_test)

svm.fit(X_train, y_train)
predictions = svm.predict(X_test)

accuracy_score(y_test, predictions)

# models = {'Linear Support Vector': LinearSVC(),
#           'Multi-layer Perceptron': MLPC(max_iter = 8), # takes foreverz :| ...
#           'Multinomial Naive Bayes': MNB(),
#           'Logistic Regression': LR(),
#           'Adaptive Boosting': ABC(n_estimators = 1000),
#           'Random Forest': RFC(n_estimators = 100)}

# for n, m in models.items():
#   m.fit(X_train, y_train)
#   predictions = m.predict(X_test)
#   print('%s : %.3f' % (n, accuracy_score(y_test, predictions)))

data_2 = data.copy(deep=True)

def getPosTags(text):
    posTags = nltk.pos_tag(nltk.word_tokenize(text))
    justTags = []
    for tags in posTags:
        justTags.append(tags[1])
    return justTags

data_2['posTags_list'] = data_2['text'].apply(lambda x: getPosTags(x))

data_2['posTags_string'] = data_2['posTags_list'].apply(lambda x: ' '.join([str(tag) for tag in x]))

# data_2['posTags_string']

data_2_V_tags = vectorizer.fit_transform(data_2['posTags_string'])
data_2_V_text = vectorizer.fit_transform(data_2['text'])

data_2_V = hstack([data_2_V_text, data_2_V_tags]).toarray()

data_2_train = pd.DataFrame(data_2_V)

X_train = data_2_train[:split].values
y_train = data_2['isClickbait'][:split].values
X_test = data_2_train[split:].values
y_test = data_2['isClickbait'][split:].values

# svm = LinearSVC()

# svm.fit(X_train, y_train)
# predictions = svm.predict(X_test)

# accuracy_score(y_test, predictions)

models = {'Linear Support Vector': LinearSVC(),
#           'Multi-layer Perceptron': MLPC(max_iter = 8), # takes foreverz :| ...
#          'Multinomial Naive Bayes': MNB(),
          'Logistic Regression': LR(),
#          'Adaptive Boosting': ABC(n_estimators = 1000),
          'Random Forest': RFC(n_estimators = 100)}

for n, m in models.items():
    print('Running %s', n)
    m.fit(X_train, y_train)
    predictions = m.predict(X_test)
    print('%s : %.3f' % (n, accuracy_score(y_test, predictions)))

pos_tags_list = ['CC', 'CD', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS','NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']

for item in pos_tags_list:
    data_2[item] = 0

for item in pos_tags_list:
    data_2[item] = data_2['posTags'].apply(lambda x: x.count(item))

data_2 = data_2.drop(columns=['V_posTags', 'V_text'])

data_2.head()



count_vectorizer = CountVectorizer()
bigram_vectorized = CountVectorizer(ngram_range=(1,2))